# AGGRESSIVE ROBOTS.TXT & NOINDEX STRATEGY
## Blocking Junk Content from Google Index

**Last Updated:** January 19, 2026  
**Purpose:** Prevent Google from wasting crawl budget on low-value pages  
**Based on:** Client feedback emphasizing aggressive junk removal  
**Impact:** Forces Google to focus ONLY on high-quality discussion threads  

---

## üéØ THE STRATEGY

### Core Principle
**"Google treats your entire domain based on what it sees first."**

If Google crawls thousands of:
- Profile pages (thin content)
- Tag archive pages (duplicate content)
- Pagination pages (duplicate content)
- Low-quality threads (1-2 word posts)

...it labels your entire domain as "thin UGC" (user-generated content) and won't index ANYTHING.

### Solution: Three-Tier Blocking
```
TIER 1: robots.txt - Block at crawl level
TIER 2: Noindex meta tags - Block at index level
TIER 3: Sitemap exclusion - Don't submit to Google
```

---

## üìã CURRENT ROBOTS.TXT STATUS

### What's Already Blocked ‚úÖ
```
# User profiles (already blocked)
Disallow: /u/
Disallow: /users/
Disallow: /p/

# Admin & system pages (already blocked)
Disallow: /admin/
Disallow: /auth/
Disallow: /email/
Disallow: /session
Disallow: /my

# Search & utility pages (already blocked)
Disallow: /search
Disallow: /g
Disallow: /badges
Disallow: /groups/

# Duplicate views (already blocked)
Disallow: /*?print=
Disallow: /*?order=
Disallow: /*?status=
Disallow: /t/*/*.rss
Disallow: /c/*.rss
```

### What's Currently MISSING ‚ùå
```
# Tag archives - NOT blocked yet
/tag/ - Thin content lists
/tags/ - Navigation pages
/tag/*/l/ - Tag list views

# Pagination - NOT blocked yet
?page= - Duplicate content
/*?*page=* - All pagination parameters

# Search results - Partially blocked
/search?q= - Could be more specific

# Activity feeds - NOT blocked
/latest - Duplicate of topics
/top - Duplicate of topics
/categories - Duplicate of category pages
```

---

## üöÄ ENHANCED ROBOTS.TXT (Client-Requested Aggressive Version)

### Complete Updated robots.txt

```robots
# ============================================================
# SANJAL.COM ROBOTS.TXT - AGGRESSIVE SEO OPTIMIZATION
# ============================================================
# Purpose: Block all junk, focus Google on quality discussions
# Updated: January 19, 2026
# ============================================================

# SITEMAP DECLARATION
# ============================================================
Sitemap: https://sanjal.com/sitemap.xml

# ============================================================
# GLOBAL RULES (All Bots)
# ============================================================
User-agent: *

# ADMIN & SYSTEM PAGES (Block completely)
# ============================================================
Disallow: /admin/
Disallow: /auth/
Disallow: /email/
Disallow: /session
Disallow: /my
Disallow: /u/
Disallow: /users/
Disallow: /user-api-key
Disallow: /p/

# SEARCH & UTILITY PAGES (Block completely)
# ============================================================
Disallow: /search
Disallow: /search?q=
Disallow: /g
Disallow: /badges
Disallow: /groups/

# TAG & CATEGORY NAVIGATION (Block archives, keep main pages)
# ============================================================
# Block tag archive pages (thin content)
Disallow: /tag/
Disallow: /tags/
Disallow: /tag/*/l/
Disallow: /tags/*/l/

# Block category list views (keep main category page)
Disallow: /c/*/l/
Disallow: /c/*/l/latest
Disallow: /c/*/l/top

# ACTIVITY FEEDS (Block - duplicate content)
# ============================================================
Disallow: /latest
Disallow: /latest.json
Disallow: /top
Disallow: /top.json
Disallow: /categories
Disallow: /categories.json
Disallow: /unread
Disallow: /new

# PAGINATION (Block all - duplicate content)
# ============================================================
# Block all pagination parameters
Disallow: /*?page=*
Disallow: /*&page=*
Disallow: /*?*page=*
Disallow: /t/*?page=*
Disallow: /c/*?page=*

# DUPLICATE VIEWS & FORMATS (Block completely)
# ============================================================
Disallow: /*?print=
Disallow: /*?order=
Disallow: /*?status=
Disallow: /*?ascending=
Disallow: /*?descending=
Disallow: /t/*/*.rss
Disallow: /c/*.rss
Disallow: /*.json
Disallow: /*.json?*

# TRACKING & API PARAMETERS (Block completely)
# ============================================================
Disallow: /*?api_key*
Disallow: /*?api_username*
Disallow: /*?utm_*
Disallow: /*?ref=*
Disallow: /*?source=*
Disallow: /*?u=*

# USER NOTIFICATIONS & PERSONAL VIEWS (Block completely)
# ============================================================
Disallow: /notifications
Disallow: /my/*
Disallow: /u/*/summary
Disallow: /u/*/activity
Disallow: /u/*/notifications
Disallow: /u/*/messages
Disallow: /u/*/preferences

# EXPLICIT ALLOWS (Ensure these CAN be crawled)
# ============================================================
Allow: /uploads/
Allow: /stylesheets/
Allow: /images/
Allow: /assets/

# Only allow main topic pages: /t/topic-slug/id
# Only allow main category pages: /c/category-slug

# ============================================================
# AI BOTS (Welcome for brand visibility)
# ============================================================

User-agent: GPTBot
Allow: /

User-agent: CCBot
Allow: /

User-agent: Google-Extended
Allow: /

User-agent: ClaudeBot
Allow: /

User-agent: anthropic-ai
Allow: /

User-agent: PerplexityBot
Allow: /

User-agent: Applebot-Extended
Allow: /

# ============================================================
# SEO SCRAPER BOTS (Block completely - waste bandwidth)
# ============================================================

User-agent: mauibot
Disallow: /

User-agent: semrushbot
Disallow: /

User-agent: ahrefsbot
Disallow: /

User-agent: blexbot
Disallow: /

User-agent: seo spider
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

# ============================================================
# END OF ROBOTS.TXT
# ============================================================
```

---

## üîß IMPLEMENTATION STEPS

### Step 1: Update robots.txt (Day 1)
**Time:** 15 minutes

**In Discourse Admin:**
```
1. Go to: Admin ‚Üí Customize ‚Üí robots.txt
2. Replace entire contents with the new version above
3. Click "Save"
4. Verify at: https://sanjal.com/robots.txt
```

**Verification:**
```
1. Visit https://sanjal.com/robots.txt in browser
2. Confirm new rules are live
3. Test with Google's robots.txt Tester:
   https://search.google.com/test/robots-txt-tester
```

### Step 2: Test Blocked URLs (Day 1)
**Time:** 30 minutes

**URLs to Test (should be blocked):**
```
‚ùå https://sanjal.com/u/username - Should be blocked
‚ùå https://sanjal.com/tag/example - Should be blocked
‚ùå https://sanjal.com/latest - Should be blocked
‚ùå https://sanjal.com/t/example/123?page=2 - Should be blocked
```

**URLs to Test (should be ALLOWED):**
```
‚úÖ https://sanjal.com/t/topic-title/123 - Should be allowed
‚úÖ https://sanjal.com/c/category-name - Should be allowed
‚úÖ https://sanjal.com/uploads/image.jpg - Should be allowed
```

### Step 3: Monitor GSC Response (Week 1-2)
**Time:** 5 min daily

**Expected Changes:**
```
Week 1-2:
‚òê Crawl requests decrease (Google stops wasting time on junk)
‚òê Crawl errors may spike temporarily (Google hits blocked URLs)
‚òê "Excluded by robots.txt" count increases

Week 3-4:
‚òê Crawl focus shifts to allowed pages
‚òê More efficient crawling of quality content
‚òê Better indexing of topic pages
```

---

## üéØ TIER 2: AGGRESSIVE NOINDEXING

### Target: Low-Quality Threads

**Criteria for Noindexing:**
```
‚òê 0-2 replies (no meaningful discussion)
‚òê < 20 total views (no interest)
‚òê Opening post < 50 words (too thin)
‚òê Spam or off-topic
‚òê Duplicate of better thread
‚òê Outdated information that can't be updated
```

### Method 1: Move to Noindex Category
**Time:** 2 hours setup + ongoing

**Setup:**
```
1. Admin ‚Üí Categories ‚Üí New Category
2. Name: "Low Value Archive" (or similar)
3. Settings:
   ‚òê Show category on homepage: ‚úó unchecked
   ‚òê Include in search: ‚úó unchecked
   ‚òê Suppress from latest: ‚úì checked
   ‚òê Visible to: Staff only (optional)
4. Save category
```

**Add to robots.txt:**
```
# Block archived low-value category
Disallow: /c/low-value-archive/
```

**Bulk Move Process:**
```
1. Identify 300-500 lowest quality threads
2. Select multiple topics (checkboxes)
3. Bulk Actions ‚Üí Move to Category
4. Select "Low Value Archive"
5. Move all at once
```

### Method 2: Close + Unlist Topics
**Time:** 1 hour

**Discourse Bulk Actions:**
```
1. Select low-quality topics
2. Bulk Actions ‚Üí Close Topics
3. Bulk Actions ‚Üí Unlist Topics

Effect:
- Closed = No new replies
- Unlisted = Removed from topic lists
- Still accessible via direct URL
- Less likely to be crawled/indexed
```

### Method 3: Direct Noindex (Requires Plugin/Theme)
**Time:** Varies

**If Available:**
```
Some Discourse themes/plugins allow per-category noindex
Check: Admin ‚Üí Themes ‚Üí [Your Theme] ‚Üí Settings
Look for: "Add noindex meta tag"
```

---

## üìä TIER 3: SITEMAP OPTIMIZATION

### Current Sitemap Status
Your sitemap at `https://sanjal.com/sitemap.xml` currently includes:
```
‚úÖ Topic pages (/t/*) - KEEP
‚úÖ Category pages (/c/*) - KEEP
‚ö†Ô∏è Tag pages (/tag/*) - CONSIDER REMOVING
‚ö†Ô∏è User profiles (/u/*) - Already in robots.txt, will be ignored
```

### Sitemap Configuration
**Time:** 15 minutes

**In Discourse Admin:**
```
1. Go to: Admin ‚Üí Settings
2. Search: "sitemap"
3. Review settings:

   enable_sitemap: ‚úì checked
   sitemap_refresh_interval: 1 (daily)
   sitemap_page_size: 50000 (default)
   
4. Ensure only quality content is included
```

**What Discourse Includes by Default:**
```
Discourse automatically excludes from sitemap:
‚úì Private categories
‚úì Unlisted topics
‚úì Closed + unlisted topics
‚úì Deleted topics
‚úì User profiles (usually)

Verify your sitemap at: https://sanjal.com/sitemap.xml
```

---

## üîç MONITORING & VALIDATION

### Week 1 Checklist
**Daily Monitoring:**
```
‚òê Check GSC ‚Üí Settings ‚Üí Crawl Stats
  - Crawl requests should decrease
  - Pages crawled per day should focus on quality

‚òê Check GSC ‚Üí Indexing ‚Üí Pages
  - "Excluded by robots.txt" should increase
  - "Crawled - not indexed" should eventually decrease

‚òê Verify robots.txt is live: sanjal.com/robots.txt
```

### Week 2-4 Checklist
**Weekly Monitoring:**
```
‚òê GSC pages indexed count
‚òê Crawl efficiency (pages crawled vs pages indexed)
‚òê Check for crawl errors
‚òê Verify important pages ARE being crawled
```

### Validation Tools

**1. Google Search Console Robots.txt Tester**
```
URL: https://search.google.com/test/robots-txt-tester
Steps:
1. Enter sanjal.com as property
2. Paste robots.txt content
3. Test specific URLs
4. Verify blocked/allowed status
```

**2. Google URL Inspection Tool**
```
GSC ‚Üí URL Inspection
Test specific URLs:
- Should see "Blocked by robots.txt" for unwanted pages
- Should see "URL is on Google" for quality pages
```

**3. Site: Search Operator**
```
In Google search:
site:sanjal.com /u/
‚Üí Should show decreasing results over time

site:sanjal.com /t/
‚Üí Should show your actual discussion topics
```

---

## ‚ö†Ô∏è EXPECTED IMPACTS & TIMELINE

### Immediate (Days 1-7)
```
‚úÖ robots.txt blocks take effect instantly
‚úÖ Google stops crawling blocked URLs
‚ö†Ô∏è GSC may show spike in crawl errors (normal - Google testing blocks)
‚ö†Ô∏è "Excluded by robots.txt" count increases
```

### Short-term (Weeks 2-4)
```
‚úÖ More efficient crawling of allowed pages
‚úÖ Crawl budget focused on quality content
‚úÖ Previously blocked pages drop from index
‚è≥ Quality pages get re-crawled more frequently
```

### Medium-term (Months 2-3)
```
‚úÖ "Crawled - not indexed" count decreases
‚úÖ More quality pages getting indexed
‚úÖ Better domain authority perception
‚úÖ Improved rankings for indexed pages
```

### Long-term (Months 4-6)
```
‚úÖ Steady stream of new indexing
‚úÖ Better organic traffic
‚úÖ Higher quality backlink profile
‚úÖ Improved user engagement metrics
```

---

## üö® TROUBLESHOOTING

### Issue 1: Important Pages Blocked
**Symptoms:** Quality content not appearing in search

**Solution:**
```
1. Check robots.txt for overly broad rules
2. Use URL Inspection in GSC
3. Add specific Allow: rule if needed
4. Request re-crawl via GSC
```

### Issue 2: Blocked URLs Still Showing in Google
**Symptoms:** Profile/tag pages still in search results

**Solution:**
```
Normal - Takes 2-4 weeks to fully remove
Speed up:
1. GSC ‚Üí Removals ‚Üí Temporary removals
2. Submit blocked URLs for removal
3. Wait for Google to process
```

### Issue 3: Crawl Rate Drops Too Much
**Symptoms:** Google barely crawling site

**Solution:**
```
1. Check GSC Crawl Stats
2. May be temporary adjustment period
3. If persists 2+ weeks, review robots.txt rules
4. Consider less aggressive blocking
```

### Issue 4: Discourse Updates Override robots.txt
**Symptoms:** Custom rules disappear after update

**Solution:**
```
1. Save robots.txt content as backup file
2. After Discourse updates, verify robots.txt
3. Re-apply custom rules if needed
4. Consider theme-level customization for persistence
```

---

## üí° ADVANCED TACTICS

### Tactic 1: Progressive Blocking
**Approach:** Start conservative, increase gradually

```
Week 1: Block profiles, admin, search
Week 2: Block tag archives
Week 3: Block pagination
Week 4: Block activity feeds

Monitor GSC impact between each change
```

### Tactic 2: Quality Threshold Enforcement
**Approach:** Only allow topics meeting criteria in sitemap

```
Criteria for sitemap inclusion:
‚òê 3+ replies minimum
‚òê 50+ views minimum
‚òê Opening post 100+ words
‚òê Not closed/unlisted
‚òê Category is public and searchable
```

**Implementation:**
```
This requires custom Discourse plugin or manual sitemap
Review: Discourse Meta for "sitemap filtering plugins"
Alternative: Use noindex category for low-quality threads
```

### Tactic 3: Crawl Delay (Use with Caution)
**Approach:** Slow down aggressive crawlers

```
User-agent: *
Crawl-delay: 1

Note: Google ignores Crawl-delay
Use only if bandwidth is issue with other bots
```

---

## üìã IMPLEMENTATION CHECKLIST

### Pre-Implementation (Day 0)
```
‚òê Backup current robots.txt
‚òê Backup Discourse settings
‚òê Document current GSC metrics
‚òê Create monitoring spreadsheet
```

### Day 1: Deploy New robots.txt
```
‚òê Update robots.txt with aggressive rules
‚òê Verify live at sanjal.com/robots.txt
‚òê Test with GSC robots.txt tester
‚òê Test sample blocked/allowed URLs
‚òê Screenshot GSC metrics (baseline)
```

### Day 2-3: Identify Low-Quality Content
```
‚òê Create "Low Value Archive" category
‚òê Audit 4,373 topics for quality
‚òê Identify 300-500 worst offenders
‚òê Document topic IDs for bulk action
```

### Day 4-5: Bulk Noindexing
```
‚òê Move 300-500 low-quality topics to archive category
‚òê Or: Close + unlist low-quality topics
‚òê Verify they're not in sitemap
‚òê Update tracking spreadsheet
```

### Day 6-7: Verification
```
‚òê Check sitemap for cleanliness
‚òê Verify robots.txt still correct
‚òê Review GSC for changes
‚òê Test random URLs for proper blocking
```

### Week 2-4: Monitor & Adjust
```
‚òê Daily: Check GSC crawl stats
‚òê Weekly: Review indexing changes
‚òê Adjust strategy based on results
‚òê Document learnings
```

---

## üéØ SUCCESS METRICS

### Track These Weekly

**Google Search Console:**
```
Metric                          | Baseline | Target (4 weeks)
--------------------------------|----------|------------------
Crawled pages per day           | ___      | ‚Üì 30-50%
"Excluded by robots.txt"        | 22       | ‚Üë 500-800
"Crawled - not indexed"         | 4,839    | ‚Üì 10-20%
Pages indexed                   | ~160     | ‚Üë 200-300
```

**Quality Indicators:**
```
‚òê Higher % of crawls on actual topic pages
‚òê Fewer crawl errors on junk pages
‚òê Better crawl efficiency ratio
‚òê More topic pages in "URL is on Google" status
```

---

## üöÄ QUICK REFERENCE

### What Gets BLOCKED (Summary)
```
‚ùå User profiles (/u/, /users/, /p/)
‚ùå Tag archives (/tag/, /tags/)
‚ùå Pagination (?page=, &page=)
‚ùå Activity feeds (/latest, /top, /categories)
‚ùå Search pages (/search)
‚ùå Admin areas (/admin/, /my/)
‚ùå Duplicate formats (.json, .rss, ?print=)
‚ùå Tracking parameters (utm_*, ?ref=)
‚ùå Personal views (/notifications, /messages)
```

### What Gets ALLOWED (Summary)
```
‚úÖ Topic pages (/t/topic-slug/id)
‚úÖ Category pages (/c/category-slug)
‚úÖ Uploads (/uploads/)
‚úÖ Assets (/stylesheets/, /images/)
‚úÖ High-quality discussions (3+ replies, good content)
```

---

## üìû SUPPORT & RESOURCES

### If You Need Help

**Discourse Meta Forum:**
```
URL: meta.discourse.org
Search for: "robots.txt SEO"
Post in: #support or #seo-and-performance
```

**Google Search Central:**
```
URL: developers.google.com/search
Focus on: robots.txt documentation
Read: Webmaster guidelines
```

**Test & Validation Tools:**
```
- GSC robots.txt tester
- GSC URL Inspection
- Technical SEO Chrome extensions
```

---

## ‚úÖ FINAL CHECKLIST

**Before Proceeding:**
```
‚òë Understood the three-tier blocking strategy
‚òë Backed up current robots.txt
‚òë Ready to monitor GSC daily for 1 week
‚òë Identified 300-500 low-quality threads
‚òë Created noindex category (or have plan to)
‚òë Have realistic expectations (4-12 week timeline)
```

**You're ready to implement when all boxes are checked!**

---

**Next Steps:**
1. Copy the new robots.txt from this document
2. Implement in Discourse admin
3. Proceed with Week 1 Day 1 tasks from the action plan
4. Monitor GSC closely for impacts

---

*Document: 5 of 5*  
*Status: Aggressive Robots.txt & Noindex Strategy*  
*Part of: GSC Analysis & Action Plan*
